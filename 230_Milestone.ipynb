{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import densenet\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow import keras\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "directory = \"/Users/sushil/Downloads/github_images\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabelsDict = {}\n",
    "\n",
    "i = 0\n",
    "for row in open('/Users/sushil/Downloads/trainLabels.csv'):\n",
    "    if (i == 0):\n",
    "        i += 1\n",
    "    else:\n",
    "        row = row.split(',')\n",
    "        trainLabelsDict[row[0] + \".jpeg\"] = int(row[1][:len(row[1]) - 1])\n",
    "print(trainLabelsDict['10_left.jpeg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "i = 0\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpeg\"): \n",
    "        if(i%50==0):\n",
    "            print(\"i: \", i)\n",
    "        image_path = os.path.join(directory, filename)\n",
    "        im = Image.open(image_path)\n",
    "        resized_image = im.resize((316,475),Image.ANTIALIAS)\n",
    "        #resized_image.save(image_path[:-5]+\"_new.jpeg\",optimize=True,quality=85) #Don't need to save\n",
    "        #print(list(resized_image.getdata())[50000])\n",
    "        X_i = np.array(list(resized_image.getdata()))\n",
    "        X_i = np.reshape(X_i,(316,475,3))\n",
    "        X_i = X_i / 255.0\n",
    "        Y_i = trainLabelsDict[filename]\n",
    "        \n",
    "        X.append(X_i)\n",
    "        Y.append(Y_i)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.reshape(np.array(Y),(1,len(Y))).T\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "nExamples = X.shape[0]\n",
    "testSplit = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:int(nExamples*(1-testSplit)),:,:]\n",
    "X_test = X[int(nExamples*(1-testSplit)):,:,:]\n",
    "Y_train = Y[:int(nExamples*(1-testSplit)),]\n",
    "Y_test = Y[int(nExamples*(1-testSplit)):,]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tf' dim-ordering\n",
    "image_dim = (316, 475, 3)\n",
    "\n",
    "model = densenet.DenseNetImageNet121(input_shape=image_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Pre-trained model our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatable plot\n",
    "# a minimal example (sort of)\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "#sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "num_class = 5\n",
    "model.layers.pop()\n",
    "model.outputs = [model.layers[-1].output]\n",
    "model.layers[-1].outbound_nodes = []\n",
    "for layer in model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "oldModel = model\n",
    "model = Sequential()\n",
    "model.add(oldModel)\n",
    "model.add(Dense(num_class, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "class_weight={0: 0.133841, 1: 1.15771, 2: 0.822009, 3: 1.60461, 4: 1.6956}\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=20, validation_split = .11, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (predictions.shape)\n",
    "def findMaxProbClass(x, axis=None):\n",
    "    y = []\n",
    "    for i in range(x.shape[0]):\n",
    "        y.append(np.argmax(x[i]))\n",
    "    return np.array(y)\n",
    "maxPredictions = findMaxProbClass(predictions)\n",
    "print(maxPredictions.T)\n",
    "print(Y_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_true=Y_test, y_pred=maxPredictions)\n",
    "print ('CONFUSION MATRIX:')\n",
    "print(confusion_matrix)\n",
    "\n",
    "print ('\\nCLASSIFICATION REPORT:')\n",
    "print(metrics.classification_report(y_true=Y_test, y_pred=maxPredictions))\n",
    "\n",
    "print ('\\nSENSITIVITY AND SPECIFICITY:')\n",
    "overallTP = 0\n",
    "overallFN = 0\n",
    "overallFP = 0\n",
    "overallTN = 0\n",
    "print('\\nPer class:')\n",
    "for i in range(len(confusion_matrix)):\n",
    "    tp = confusion_matrix[i][i] #diagonal\n",
    "    fn = np.sum(confusion_matrix[i]) - tp\n",
    "    fp = np.sum(confusion_matrix.T[i]) - tp\n",
    "    tn = np.sum(confusion_matrix) - tp - fp - fn\n",
    "    overallTP += tp\n",
    "    overallFN += fn\n",
    "    overallFP += fp\n",
    "    overallTN += tn\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    print('p: ' + str(precision) + '\\tr: ' + str(recall))\n",
    "    print('sensitivity: ' + str(sensitivity) + '\\tspecifity: ' + str(specificity))\n",
    "print('\\nOverall (micro avg.) :')\n",
    "microAvgSensitivity = overallTP/(overallTP + overallFN)\n",
    "microAvgSpecificity = overallTN/(overallTN + overallFP)\n",
    "print('sensitivity: ' + str(microAvgSensitivity) + '\\tspecifity: ' + str(microAvgSpecificity))\n",
    "\n",
    "print ('\\nACCURACY PER CLASS: ')\n",
    "cm = confusion_matrix\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())\n",
    "\n",
    "print ('\\nACCURACY: ')\n",
    "print(metrics.accuracy_score(y_true=Y_test, y_pred=maxPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Model Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.pickle'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=3, activation='relu', input_shape=(316,475,3)))\n",
    "#model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(5, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=50, validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
